<?xml version="1.0" encoding="UTF-8"?>
<xmldoc>
  <copyright>SAME AS NELSON SOFTWARE</copyright>

  <language>en_US</language>
  <keyword>MPI_Allreduce</keyword>
  <short_description>Combines values from all processes and distributes the result back to all processes.</short_description>

  <syntax>
    <syntax_item>r = MPI_Allreduce(Value, Operation, Comm)</syntax_item>
  </syntax>

  <param_input>
    <param_input_item>
      <param_name>Value</param_name>
      <param_description>value to send: numeric or logical array (sparse not supported).</param_description>
    </param_input_item>

    <param_input_item>
      <param_name>Operation</param_name>
      <param_description>a string: MPI_SUM, MPI_MAX, MPI_MIN, MPI_SUM, MPI_PROD, MPI_LAND, MPI_LOR, MPI_BAND, MPI_BOR, MPI_LXOR or MPI_BXOR</param_description>
    </param_input_item>

    <param_input_item>
      <param_name>Comm</param_name>
      <param_description>a MPI_Comm object.</param_description>
    </param_input_item>
  </param_input>


  <param_output>
    <param_output_item>
      <param_name>r</param_name>
      <param_description>received value</param_description>
    </param_output_item>
  </param_output>

  <description>
  <p>Combines values from all processes and distributes the result back to all processes.</p>
  <p>Nelson does not check to ensure that the reduction operation are all the same size across the various processes in the group.</p>
  <p>Please be sure that each process passes the same sized array to the MPI_Allreduce operation.</p>
</description>

  <used_function></used_function>
  <bibliography></bibliography>

  <see_also>
    <see_also_item>
      <link linkend="${mpi}MPI_Reduce">MPI_Reduce</link>
    </see_also_item>
  </see_also>


<examples>

  <example_item>
    <example_item_type>nelson</example_item_type>
    <example_item_description>mpiexec([modulepath('mpi'), '/examples/help_examples/MPI_Allreduce.m'], 4)</example_item_description>
    <example_item_data><![CDATA[
if ~MPI_Initialized()
  MPI_Init();
end
comm = MPI_Comm_object();
my_rank = MPI_Comm_rank ();
num_ranks = MPI_Comm_size();

A = [1 + my_rank:3 + my_rank]
B = MPI_Allreduce(A, 'MPI_PROD', comm);
if (my_rank == 0)
  disp('Result:')
  disp(B);
end
if MPI_Initialized()
  MPI_Finalize();
end
]]>
    </example_item_data>
  </example_item>

</examples>


  <history>
    <history_item>
      <history_version>1.0.0</history_version>
      <history_description>initial version</history_description>
    </history_item>
  </history>

  <authors>
    <author_item>Allan CORNET</author_item>
  </authors>
</xmldoc>
